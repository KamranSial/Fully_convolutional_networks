{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "from matplotlib import colors\n",
    "import fcn32_new as fcn32_vgg\n",
    "import csv\n",
    "ckpt_dir = \"/mnt/data1/fcn/ckpt_dir\"\n",
    "PRETRAINED_MODEL_PATH= \"/mnt/data1/fcn/ckpt_dir\"\n",
    "LOGS_PATH = '/home/sik4hi/tensorflow_logs'\n",
    "WEIGHT_PATH = '.npy'\n",
    "TRAINSET_PATH = '/mnt/data1/city/csv_files/cityscapes_train_wmask.csv'\n",
    "VALSET_PATH = '/mnt/data1/city/csv_files/cityscapes_val_wmask.csv'\n",
    "\n",
    "NUM_OF_CLASSESS = 19\n",
    "BATCH_SIZE = 1\n",
    "IMAGE_HEIGHT = 1024\n",
    "IMAGE_WIDTH = 2048\n",
    "NUM_CHANNELS = 3\n",
    "N_EPOCHS = 300\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "f=open(TRAINSET_PATH,\"r\")\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "masks=[]\n",
    "for line in f:\n",
    "    filepath, label, mask= line.split(\",\")\n",
    "    filepaths.append(filepath)\n",
    "    labels.append(label)\n",
    "    masks.append(mask.split(\"\\r\")[0])\n",
    "    \n",
    "one_ex = tf.train.slice_input_producer([filepaths,labels, masks]\n",
    "                                           ,capacity = 2975)\n",
    "#csv_path = tf.train.string_input_producer([TRAINSET_PATH])\n",
    "#textReader = tf.TextLineReader()\n",
    "#_, content = textReader.read(csv_path)\n",
    "#one_ex = tf.decode_csv(content, record_defaults=[[\"\"], [\"\"], [\"\"]])\n",
    "\n",
    "im_content = tf.read_file(one_ex[0])\n",
    "train_image = tf.image.decode_png(im_content, channels=3)\n",
    "train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "train_image = tf.cast(train_image, tf.float32)\n",
    "\n",
    "la_content = tf.read_file(one_ex[1])\n",
    "label_image = tf.image.decode_png(la_content, channels=1)\n",
    "label_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, 1])\n",
    "label_image=tf.squeeze(label_image, squeeze_dims=[2])\n",
    "\n",
    "#record_bytes=2097152\n",
    "#reader=tf.FixedLengthRecordReader(record_bytes)\n",
    "value =tf.read_file(one_ex[2])\n",
    "mask=tf.decode_raw(value,tf.uint8)\n",
    "mask.set_shape([2097152])\n",
    "#mask=tf.cast(mask, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_image_batch, train_label_batch, train_mask_batch= tf.train.batch([train_image, label_image, mask], batch_size=BATCH_SIZE,\n",
    "                                                               capacity=100 + 3 * BATCH_SIZE,\n",
    "                                                               num_threads=7)\n",
    "with tf.device('/gpu:0'):\n",
    "    sess = tf.Session()\n",
    "    #images_tf = tf.placeholder(tf.float32,[None, 1024, 2048, 3])\n",
    "    #labels_tf = tf.placeholder(tf.int32,[None, 1024, 2048])\n",
    "    #weights = tf.placeholder(tf.float32,[None, 1024, 2048])\n",
    "\n",
    "    vgg_fcn = fcn32_vgg.FCN32VGG('./vgg16.npy')\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg_fcn.build(train_image_batch, train=True, num_classes=19, random_init_fc8=True, debug=False)\n",
    "\n",
    "\n",
    "    #head=[]\n",
    "    #sum_of_weights=tf.reduce_sum(weights)\n",
    "    #labels_oh= tf.one_hot(labels_tf,19)\n",
    "    labels_tf_flat=tf.reshape(tf.mul(tf.reshape(train_label_batch,[-1]),train_mask_batch),[-1])\n",
    "    mask2=tf.reshape(train_mask_batch,[1024,2048])\n",
    "    #labels_tf_flat2= tf.gather(labels_tf_flat,tf.where(tf.not_equal(labels_tf_flat,19))[1])\n",
    "    logits = tf.reshape(vgg_fcn.upscore, (-1, NUM_OF_CLASSESS))\n",
    "    #logits = tf.gather(logits,tf.where(tf.not_equal(labels_tf_flat,19)))\n",
    "    labels_tf_flat = tf.cast(labels_tf_flat, tf.int32)\n",
    "\n",
    "    #epsilon = tf.constant(value=1e-4)\n",
    "    #logits = logits + epsilon\n",
    "    #labels_flat_sparse = tf.reshape(labels_oh, (-1, NUM_OF_CLASSESS))\n",
    "    #labels_flat_sparse=tf.cast(labels_flat_sparse, tf.float64)\n",
    "    #ww_flat=tf.reshape(weights,[-1])\n",
    "    #ww_flat = tf.cast(ww_flat, tf.float64)\n",
    "    #softmax = tf.nn.softmax(logits)\n",
    "    #softmax = tf.cast(softmax, tf.float64)\n",
    "    #test1=-tf.reduce_sum(labels_flat_sparse * tf.log(softmax), reduction_indices=[1])\n",
    "    #test2=tf.reduce_sum(test1)\n",
    "    #cross_entropy = tf.mul(-tf.reduce_sum(labels_flat_sparse * tf.log(softmax), reduction_indices=[1]),ww_flat)\n",
    "    #cross_entropy = -tf.reduce_sum(labels_flat_sparse * tf.log(softmax), reduction_indices=[1])\n",
    "    #mask_flat=tf.reshape(train_mask_batch,[-1])\n",
    "    train_mask_batch=tf.cast(train_mask_batch, tf.float32)\n",
    "    cross_entropy =tf.mul(tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels_tf_flat),train_mask_batch)\n",
    "    #cross_entropy_sum= tf.reduce_sum(cross_entropy,\n",
    "    #                                name='xentropy_mean')\n",
    "    #cross_entropy_mean = tf.reduce_mean(cross_entropy,name='xentropy_mean')\n",
    "    #cross_entropy_mean = cross_entropy_sum/tf.cast(sum_of_weights, tf.float32)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,\n",
    "                                        name='xentropy_mean')\n",
    "    l2_loss=tf.reduce_sum(tf.get_collection(\"losses\"))\n",
    "    total_loss=cross_entropy_mean +l2_loss\n",
    "    #loss_tf = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(vgg_fcn.upscore,\n",
    "     #                                                                     tf.squeeze(labels_tf, squeeze_dims=[3]),\n",
    "      #                                                                    name=\"entropy\")))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #train_op = tf.train.MomentumOptimizer(0.0001, 0.99).minimize(cross_entropy_mean)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(total_loss)\n",
    "    predictions=tf.argmax(logits, 1)\n",
    "    #labels_tf_flat=tf.reshape(labels_tf_flat,[-1])\n",
    "    #labels_flat=tf.cast(labels_tf_flat, tf.int64)\n",
    "    pred_flat=tf.reshape(predictions,[-1])\n",
    "with tf.device('/cpu:0'):\n",
    "    miou=tf.contrib.metrics.streaming_mean_iou(pred_flat,labels_tf_flat,19,weights=train_mask_batch)\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "with tf.device('/gpu:0'):\n",
    "    print('Finished building Network.')\n",
    "    \n",
    "    init_op = tf.group(tf.initialize_all_variables(),\n",
    "                       tf.initialize_local_variables())\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    if PRETRAINED_MODEL_PATH:\n",
    "        print \"using Pretrained model\"\n",
    "        ckpt = tf.train.get_checkpoint_state(PRETRAINED_MODEL_PATH)\n",
    "        print(ckpt)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        \n",
    "    # print(csv_path)\n",
    "    # For populating queues with batches, very important!\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_train_loss_list,train_loss_list,plot_train_loss,plot_total_train_loss,plot_miou,loss= [],[],[],[],[],0\n",
    "#for epoch in xrange(N_EPOCHS):\n",
    "cmap = colors.ListedColormap(['blue','lightblue','orange','purple','r'\n",
    "                              ,'darkkhaki','yellow','saddlebrown','orchid','grey'\n",
    "                              ,'hotpink','blueviolet','gold','deeppink','greenyellow'\n",
    "                              ,'maroon','yellowgreen','green','orangered','black'])\n",
    "norm=colors.NoNorm(0,19)\n",
    "try:\n",
    "    while not coord.should_stop() and epoch< N_EPOCHS:\n",
    "        epoch+=1\n",
    "        if(math.isnan(loss)):\n",
    "                break\n",
    "        epoch_start_time = time.time()\n",
    "        for iteration in xrange(2975/BATCH_SIZE):   \n",
    "            #train_imbatch, train_labatch = sess.run([train_image_batch, train_label_batch])\n",
    "            #image1=copy.deepcopy(train_labatch[0])\n",
    "            #ww=[[[0 for k in range(2048)] for j in range(1024)]for i in range(BATCH_SIZE)]\n",
    "            #sum2=0\n",
    "            #for i in range(BATCH_SIZE):\n",
    "            #    for j in range(1024):\n",
    "            #        for k in range(2048):\n",
    "            #            if(train_labatch[i][j][k]==19):\n",
    "            #                ww[i][j][k]=0\n",
    "            #                train_labatch[i][j][k]=18\n",
    "            #            else:\n",
    "            #                ww[i][j][k]=1\n",
    "                            #sum2+=1\n",
    "           # sum2=sess.run([sum_of_weights],\n",
    "           #         feed_dict={weights: ww})\n",
    "            if (iteration%500==0):\n",
    "                _,loss,tloss,iou,output, output3 = sess.run([train_op,cross_entropy_mean,total_loss,\n",
    "                                                            miou,vgg_fcn.pred_up,train_label_batch])\n",
    "                plt.figure(1)\n",
    "                plt.imshow(output3[0],cmap=cmap,norm=norm)\n",
    "                plt.figure(2)\n",
    "                plt.imshow(output[0],cmap=cmap,norm=norm)\n",
    "                #plt.figure(3)\n",
    "                #plt.imshow(output2[0],cmap=cmap,norm=norm)\n",
    "                plt.show()\n",
    "                \n",
    "            else:\n",
    "                _,loss,tloss,iou,output,output2 = sess.run([train_op,cross_entropy_mean,total_loss,\n",
    "                                                            miou,vgg_fcn.pred_up,vgg_fcn.pred])\n",
    "\n",
    "            if(math.isnan(loss)):\n",
    "                break\n",
    "            train_loss_list.append(loss)\n",
    "            total_train_loss_list.append(tloss)\n",
    "            sys.stdout.write('\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r' + \"Iteration:\" + str(iteration)+ \"  Loss: \"+ str(loss)+ \" IOU: \"+ str(iou[0]))\n",
    "            #print(loss)\n",
    "            #print(val1)\n",
    "            #print(val2)\n",
    "            #print sum2\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            #if (iteration%500==0):\n",
    "                \n",
    "\n",
    "\n",
    "        tl = np.mean(train_loss_list)\n",
    "        ttl=np.mean(total_train_loss_list)\n",
    "        plot_train_loss.append(tl)\n",
    "        plot_total_train_loss.append(ttl)\n",
    "\n",
    "        train_loss_list = []\n",
    "        total_train_loss_list = []\n",
    "        plot_miou.append(iou[0])\n",
    "        \n",
    "        var_miou = filter(lambda x: x.name.endswith('total_confusion_matrix:0'), tf.local_variables())\n",
    "        init_miou=tf.initialize_variables(var_miou)\n",
    "        sess.run(init_miou)\n",
    "        \n",
    "        clear_output()\n",
    "        print \"===========**Training ACCURACY**================\"\n",
    "        print \"Epoch\", epoch + 1\n",
    "        print 'Training mIOU: ', iou[0]\n",
    "        print \"Training Loss:\", tl \n",
    "        print \"Training Loss:\", ttl \n",
    "        print 'Time Elapsed for Epoch:' + str(epoch) + ' is ' + str(\n",
    "                (time.time() - epoch_start_time) / 60.) + ' minutes'\n",
    "\n",
    "        plt.figure(1) \n",
    "        dd = plt.plot(plot_total_train_loss,'r',label=\"Training\")\n",
    "        plt.title(\"Total LOSS\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.figure(2) \n",
    "        aa = plt.plot(plot_train_loss,'r',label=\"Training\")\n",
    "        plt.title(\"LOSS\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.figure(3)\n",
    "        cc = plt.plot(plot_miou,'r',label=\"Training\")\n",
    "        plt.title(\"mIOU\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        saver.save(sess, ckpt_dir + \"/model.ckpt\", global_step=epoch)\n",
    "        ofile  = open('data.csv', \"a\")\n",
    "        writer = csv.writer(ofile)\n",
    "        timep=((time.time() - epoch_start_time) / 60.)\n",
    "        writer.writerow([tl,ttl,iou[0],timep,epoch])\n",
    "        ofile.close()\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print (\"out of range\")\n",
    "finally:\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    print(\"requesting stop\")\n",
    "        #print(iou[1])\n",
    "            #train_loss= sess.run(vgg_fcn.upscore,\n",
    "             #   feed_dict={ images_tf: train_imbatch})\n",
    "            #res=sess.run(image)\n",
    "            #plt.imshow(train_labatch[0])\n",
    "            #plt.show()\n",
    "    #    aa= tt * np.log10(train_loss)\n",
    "    #    print(len(aa))\n",
    "    #    print(len(aa[0]))\n",
    "    #print(sum)\n",
    "    #print(sum2)\n",
    "    #print(len(ww))\n",
    "    #print(len(ww[0]))\n",
    "    #print(len(ww[0][0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmap = colors.ListedColormap(['blue','purple','orange','lightblue','r'\n",
    "                              ,'darkkhaki','yellow','green','orchid','grey'\n",
    "                              ,'hotpink','blueviolet','gold','deeppink','greenyellow'\n",
    "                              ,'maroon','yellowgreen','saddlebrown','orangered','black'])\n",
    "norm=colors.NoNorm(0,19)\n",
    "output,output2,log= sess.run([train_label_batch,mask2,logits])\n",
    "print(len(log))\n",
    "plt.figure(1)\n",
    "plt.imshow(output[0],cmap=cmap,norm=norm)\n",
    "plt.figure(3)\n",
    "plt.imshow(output2,cmap=cmap,norm=norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_labatch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
